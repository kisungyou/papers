---
title: "Simulated Example: Model Reduction"
author: "Kisung You"
format:
  gfm:
    toc: true
    toc-depth: 2
execute:
  echo: true
  warning: false
  message: false
  freeze: auto
knitr:
  opts_chunk:
    dpi: 400
---


## Setup

This notebook is to replicate the second simulated example where the ground truth 
is a mixture of 4 vMF distributions. We start from fitting a model with larger 
number of component. Then, the proposed WL-based model reduction methods are applied 
in order to verify whether the methods are actually capable of identifying 
the small number of components correctly without accessing the original training data.


```{r setup}
# Load packages
library(pacman)
pacman::p_load(maotai,
               scales,
               viridisLite)

# Helper functions
source("auxiliary.R")
```


## Data Generation

We will generate 4 vMF distributions whose locations are equidistant to each other. For the "east" and "west" directions, we put high concentration. The "north" and "south" are with low $\kappa$ values.

```{r data-generate, out.width="90%", fig.align="center"}
# basic setting
vec_mu = c(0, pi/2, pi, 3*pi/2)
small_kappa = 10
large_kappa = 10
mat_mu = array(0,c(length(vec_mu),2))
for (i in 1:length(vec_mu)){
  now_mu = vec_mu[i]
  mat_mu[i,1] = cos(now_mu)
  mat_mu[i,2] = sin(now_mu)
}

# randomly generate: east and west are high kappa, north and south are small kappa
n_per_group = 100
pts_north = vMF::rvMF(n_per_group, mat_mu[1,]*small_kappa)
pts_east  = vMF::rvMF(n_per_group, mat_mu[2,]*large_kappa)
pts_south = vMF::rvMF(n_per_group, mat_mu[3,]*small_kappa)
pts_west  = vMF::rvMF(n_per_group, mat_mu[4,]*large_kappa)
pts_all   = rbind(pts_north, pts_east, pts_south, pts_west)

# draw the exact distributions
grid_angle  = seq(from=0, to=2*pi, length.out=1000)
wow_density = aux_density2_mix(grid_angle, mat_mu, c(small_kappa, large_kappa, small_kappa, large_kappa))
pts_density = wow_density$coords
max_density = wow_density$whichK

par(pty="s", mfrow=c(1,2))
# density
plot(pts_density[,1], pts_density[,2], type="l", lwd=2, asp=1, 
     xlab="Dimension 1", ylab="Dimension 2", main="Density",
     xaxt="n", yaxt="n")
lines(cos(grid_angle), sin(grid_angle))

# points
plot(pts_density[,1], pts_density[,2], type="l", lwd=1, asp=1, col="#0000FF00",
     xlab="Dimension 1", ylab="Dimension 2", main="Sampled Points",
     xaxt="n", yaxt="n")
lines(cos(grid_angle), sin(grid_angle))
points(pts_all[,1], pts_all[,2], pch=19, col=rep(1:4, each=n_per_group), cex=.5)
```

## Fit

The true data-generating process has $K=4$ components. We fit the mixture model with 
vMF distributions across varying number of components from $K=2$ to $K=10$ where the 
expectation is that the model with $K=4$ should be selected as the best one. For the 
reduction methods based on greedy optimization and partitional methods---$k$-medoids and hierarchical clustering--- will start from a larger model ($K=10$) and reduce sequentially up to the smallest ($K=2$) model. 

```{r fit-base}
# filename to save or load
file_name = paste0("computed_fits_",small_kappa,"_",large_kappa,".RData")
if (file.exists(file_name)){
  load(file_name)
} else{
  # empty vector of list to save computed outputs
  fits_exact  = vector("list", length=9)
  fits_greedy = vector("list", length=9)
  fits_hclust = vector("list", length=9)
  fits_kmedoids = vector("list", length=9)
  
  # fit exact
  for (i in 1:9){
    fits_exact[[i]] = aux_movMF_exact(pts_all, K=(i+1))
  }
  
  # fit others
  fits_greedy[[9]] <- fits_hclust[[9]] <- fits_kmedoids[[9]] <- fits_exact[[9]]
  for (i in 1:8){
    fits_greedy[[i]] = aux_movMF_reduce(pts_all, big.K = 10, small.K = i+1,use.greedy=TRUE)
    fits_hclust[[i]] = aux_movMF_reduce(pts_all, big.K = 10, small.K = i+1, use.greedy = FALSE, use.hclust = TRUE)
    fits_kmedoids[[i]] = aux_movMF_reduce(pts_all, big.K = 10, small.K = i+1, use.greedy = FALSE, use.hclust = FALSE)
  }
  
  save(fits_exact, fits_greedy, fits_hclust, fits_kmedoids, file=file_name)
}
```

Since all models are learned, now we compare their performances by extracting 
information criteria values. In the manuscript, only the Bayesian information criterion (BIC) metrics are reported since BIC is consistent, not prone to favor larger models as AIC does, and seems to be standard for reporting in the mixture modeling literature. For fair comparison, here we also provide Akaike information criterion (AIC) and Hannan-Quinn information criterion (HQIC) values for reference.

```{r extract-ic}
# extract {A,B,HQ}IC values
vals_BIC <- array(0,c(9,4))
vals_AIC <- array(0,c(9,4))
vals_HQIC <- array(0,c(9,4))
for (i in 1:9){
  vals_BIC[i,1] <- fits_exact[[i]]$BIC
  vals_BIC[i,2] <- fits_greedy[[i]]$BIC
  vals_BIC[i,3] <- fits_hclust[[i]]$BIC
  vals_BIC[i,4] <- fits_kmedoids[[i]]$BIC
  
  vals_AIC[i,1] <- fits_exact[[i]]$AIC
  vals_AIC[i,2] <- fits_greedy[[i]]$AIC
  vals_AIC[i,3] <- fits_hclust[[i]]$AIC
  vals_AIC[i,4] <- fits_kmedoids[[i]]$AIC
  
  vals_HQIC[i,1] <- fits_exact[[i]]$HQIC
  vals_HQIC[i,2] <- fits_greedy[[i]]$HQ
  vals_HQIC[i,3] <- fits_hclust[[i]]$HQIC
  vals_HQIC[i,4] <- fits_kmedoids[[i]]$HQ
}
```

## Visualize

Now we present a single figure that summarizes the performance of different reduction tools.

```{r plot-ic, out.width="90%", fig.align="center"}
par(mfrow=c(1,3), pty="s")

# graphical settings
cex_main  = 1.7
cex_lab   = 1.5
cex_axis  = 1.25
linewidth = 0.5
cex_pts   = 0.5
cex_leg   = 1

# AIC
turbo_scale = viridis::turbo(4)
matplot(log(vals_AIC), type="b", pch=c(19, 17, 15, 3), lty=1, col=turbo_scale, xaxt="n",
        xlab="Number of Components", ylab="log(AIC)",
        cex.axis=cex_axis, cex.lab=cex_lab, lwd=1.5)
title(main="AIC", family="sans", cex.main=cex_main)
legend("topright", legend = c("Exact", "Greedy", "Hclust", "Kmedoids") , col = turbo_scale, 
       pch = c(19, 17, 15, 3), lty = 1, lwd = 1.5, cex=cex_leg)
axis(1, at = 1:9, labels = 2:10, cex.axis=cex_axis)

# BIC
turbo_scale = viridis::turbo(4)
matplot(log(vals_BIC), type="b", pch=c(19, 17, 15, 3), lty=1, col=turbo_scale, xaxt="n",
        xlab="Number of Components", ylab="log(BIC)",
        cex.axis=cex_axis, cex.lab=cex_lab, lwd=1.5)
title(main="BIC", family="sans", cex.main=cex_main)
legend("topright", legend = c("Exact", "Greedy", "Hclust", "Kmedoids") , col = turbo_scale, 
       pch = c(19, 17, 15, 3), lty = 1, lwd = 1.5, cex=cex_leg)
axis(1, at = 1:9, labels = 2:10, cex.axis=cex_axis)

# HQIC
turbo_scale = viridis::turbo(4)
matplot(log(vals_HQIC), type="b", pch=c(19, 17, 15, 3), lty=1, col=turbo_scale, xaxt="n",
        xlab="Number of Components", ylab="log(HQIC)",
        cex.axis=cex_axis, cex.lab=cex_lab, lwd=1.5)
title(main="HQIC", family="sans", cex.main=cex_main)
legend("topright", legend = c("Exact", "Greedy", "Hclust", "Kmedoids") , col = turbo_scale, 
       pch = c(19, 17, 15, 3), lty = 1, lwd = 1.5, cex=cex_leg)
axis(1, at = 1:9, labels = 2:10, cex.axis=cex_axis)
```

As the paper discusses, $K=4$ shows drastic patterns for reduction methods, being an *elbow* point in terms of all information criteria. While not achieving the lowest value, at least it provides a significant hint at structural properties of a plausible range for the number of  mixture components.